{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTranslation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n00blet/OpenNMT-Machine-Translation/blob/master/notebook/NeuralMachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eNvH4AbDxDj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "-zqGQjlFxRwG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Neural Machine Translation using Seq2Seq Encoder-Decoder**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Language translation has been an open problem for many years now. Even though there are many tools like Google translate, DeepL they are still evolving and far near perfect to human translation. And languages in general are vast, as in a statement can be expressed or written in more than one way. In this tutorial we will try to build a simple and basic machine translation system using [OpenNMT](http://opennmt.net/).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Pre-Requisite **\n",
        "\n",
        "This article assumes that you have some basic knowledge in Python programming and Neural Machine Translation.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "*  Python Virtual Environment (Recommended)\n",
        "\n",
        "*  [TensorFlow](https://www.tensorflow.org/install) (GPU version)\n",
        "\n",
        "*   [CUDA](https://developer.nvidia.com/cuda-downloads) compatible Nvidia Graphics Card\n",
        "\n",
        "* [OpenNMT](https://github.com/OpenNMT/OpenNMT-tf)\n",
        "\n",
        "\n",
        "*   [Sentence Piece](https://github.com/google/sentencepiece#c-from-source)(Recommended Installation from Source)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Overview**\n",
        "\n",
        "\n",
        "*   PreProcessing the Data\n",
        "*   Training with different hyperparameters\n",
        "*   Inference or Translation\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Diving into OpenNMT**\n",
        "\n",
        "For the tasks further below, we will download one of the dataset from Machine Translation Research websites [WMT Translation Task](http://www.statmt.org/wmt16/translation-task.html)  or  [OPUS Parallel Corpus](http://opus.nlpl.eu/) .\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Step 1: Preprocessing**\n",
        "\n",
        "Assuming that we have our environment ready and configured for training using GPU, the first step in this task is to preprocess the raw data.\n",
        "\n",
        "Here we build **source** and **target** word vocabularies using an Unsupervised Text Tokenizer [SentencePiece](https://github.com/google/sentencepiece). \n",
        "\n",
        "<br>\n",
        "\n",
        "Before we go into the next task, we can split the dataset(news-commentary) into **train**, **test** and **validation** using SkLearn or we can use the validation and test set from data folder. \n",
        " \n",
        "Read more about train-test-split here [Data Splitting](https://cs230-stanford.github.io/train-dev-test-split.html).\n",
        "\n",
        "Now, let's go and train a sentencepiece model for our dataset. We will use this model to generate vocabs and to tokenize all the files (train,test and val).\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uu-BkvhS4NFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spm_train --input=news-commentary-v11.de-en.en --model_prefix=english --vocab_size=32000 --character_coverage=1.0 --model_type=bpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVqQIFuLJd22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spm_train --input=news-commentary-v11.de-en.de --model_prefix=german --vocab_size=32000 --character_coverage=1.0 --model_type=bpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wmKNQ4c0Jafd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "After successfully running those two commands, following four files should have been generated.\n",
        "\n",
        "*   english.model\n",
        "*   english.vocab\n",
        "*   german.model\n",
        "*   german.vocab\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sn58B_lnKngv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# Task 1\n",
        "\n",
        "If you look at the english.vocab, "
      ]
    },
    {
      "metadata": {
        "id": "ontoDFPL9HdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\\\\n",
        "\n",
        "**Step 2: Training  and Evaluating  the data**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Pf4vC-BZ9LA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AfyLrQcb9Nqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 3: Traslation and Testing the results**"
      ]
    },
    {
      "metadata": {
        "id": "Ks3XEkNg9Pkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}