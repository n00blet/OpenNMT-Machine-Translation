{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTranslation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n00blet/OpenNMT-Machine-Translation/blob/master/notebook/NeuralMachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eNvH4AbDxDj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "-zqGQjlFxRwG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Neural Machine Translation using Seq2Seq Encoder-Decoder**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Language translation has been an open problem for many years now. Even though there are many tools like Google translate, DeepL they are still evolving and far near perfect to human translation. And languages in general are vast, as in a statement can be expressed or written in more than one way. In this tutorial we will try to build a simple and basic machine translation system using [OpenNMT](http://opennmt.net/).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Pre-Requisite **\n",
        "\n",
        "This article assumes that you have some basic knowledge in Python programming and Neural Machine Translation.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "*  Python Virtual Environment (Recommended)\n",
        "\n",
        "*  [TensorFlow](https://www.tensorflow.org/install) (GPU version)\n",
        "\n",
        "*   [CUDA](https://developer.nvidia.com/cuda-downloads) compatible Nvidia Graphics Card\n",
        "\n",
        "* [OpenNMT](https://github.com/OpenNMT/OpenNMT-tf)\n",
        "\n",
        "\n",
        "*   [Sentence Piece](https://github.com/google/sentencepiece#c-from-source)(Recommended Installation from Source)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Overview**\n",
        "\n",
        "\n",
        "*   PreProcessing the Data\n",
        "*   Training with different hyperparameters\n",
        "*   Inference or Translation\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Diving into OpenNMT**\n",
        "\n",
        "For the tasks further below, we will download one of the dataset from Machine Translation Research websites [WMT Translation Task](http://www.statmt.org/wmt16/translation-task.html)  or  [OPUS Parallel Corpus](http://opus.nlpl.eu/) .\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Step 1: Preprocessing**\n",
        "\n",
        "Assuming that we have our environment ready and configured for training using GPU, the first step in this task is to preprocess the raw data.\n",
        "\n",
        "Here we build **source** and **target** word vocabularies using an Unsupervised Text Tokenizer [SentencePiece](https://github.com/google/sentencepiece). \n",
        "\n",
        "<br>\n",
        "\n",
        "Before we go into the next task, we can split the dataset(news-commentary) into **train**, **test** and **validation** using SkLearn or we can use the validation and test set from data folder. \n",
        " \n",
        "Read more about train-test-split here [Data Splitting](https://cs230-stanford.github.io/train-dev-test-split.html).\n",
        "\n",
        "Now, let's go and train a sentencepiece model for our dataset. We will use this model to generate vocabs and to tokenize all the files (train,test and val).\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uu-BkvhS4NFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spm_train --input=news-commentary-v11.de-en.en --model_prefix=english --vocab_size=32000 --character_coverage=1.0 --model_type=bpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVqQIFuLJd22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spm_train --input=news-commentary-v11.de-en.de --model_prefix=german --vocab_size=32000 --character_coverage=1.0 --model_type=bpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wmKNQ4c0Jafd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "After successfully running those two commands, following four files should have been generated.\n",
        "\n",
        "*   english.model\n",
        "*   english.vocab\n",
        "*   german.model\n",
        "*   german.vocab\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sn58B_lnKngv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# Task 1\n",
        "\n",
        "The first task is to tokenize each of the sentence from English and German dataset using the model generated by sentencepiece and save it to another file.\n",
        "\n",
        "<br>\n",
        "For example *train_en_tok.txt* , *train_de_tok.txt*\n",
        "\n",
        "<br>\n",
        "\n",
        "Hint : You can also use  **spm_encode**  to fininsh the task."
      ]
    },
    {
      "metadata": {
        "id": "6ODUnVWgVj6j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "def tokenize():\n",
        "  #Complete the function below\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cs5nBR0Kav51",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "If you look at the validation and test files, the main content are encoded within SGML tags. \n",
        "\n",
        "<br>\n",
        "\n",
        "Or if you don't want to write the code, you can look at the existing perl script [here](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/input-from-sgm.perl).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DykLyzskgPDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_tags():\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vp_HpTG6iVqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we have tokenized all the six files, let us build our configuration file for training using OpenNMT.\n",
        "\n",
        "<br>\n",
        "Let us create *data.yml* file with the following configuration, this can be changed anytime based on your task and requirement.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model_dir: en_de_translation\n",
        "\n",
        "data:\n",
        "  train_features_file: train.en    #tokenized english data\n",
        "  train_labels_file: train.de      #tokenized german data\n",
        "  eval_features_file: valid.en     #tokenized english val data\n",
        "  eval_labels_file: valid.de       #tokenized german val data\n",
        "  \n",
        "  source_words_vocabulary: english.vocab   #vocab generated by sp\n",
        "  target_words_vocabulary: german.vocab\n",
        "\n",
        "train:\n",
        "  save_checkpoints_steps: 1000\n",
        "  \n",
        "  #Keeping 10 checkpoint in storage\n",
        "  keep_checkpoint_max: 10\n",
        "  \n",
        "  train_steps : 200000\n",
        "  batch_size : 32\n",
        " \n",
        "params:\n",
        "  #If the model is not able to find the actual word, it tries replacing with the nearest possible word\n",
        "  replace_unknown_target: True\n",
        "  optimizer: AdamOptimizer\n",
        "  learning_rate : 0.001\n",
        "  \n",
        "\n",
        "eval:\n",
        "  #Perform evaluation after every 10 minutes (helps in Tensorboard loss log)\n",
        "  eval_delay: 600\n",
        "  save_eval_predictions: True\n",
        "  \n",
        "\n",
        "infer:\n",
        "batch_size: 32\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "Now that we have all the files ready, let's go ahead and train our model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ontoDFPL9HdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\\\\n",
        "\n",
        "**Step 2: Training  and Evaluating  the data**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Pf4vC-BZ9LA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "onmt-main train_and_eval --model_type Transformer --auto_config --config data.yml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AfyLrQcb9Nqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 3: Translation and Testing the results**"
      ]
    },
    {
      "metadata": {
        "id": "Ks3XEkNg9Pkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "onmt-main infer --auto_config --config data.yml --features_file test.en --predictions_file output.de"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fklvp6aSn6Bw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Infer will take the test file and tries to predict a relevant translation and saves it to output.de. \n",
        "The predictions might not be very accurate as the dataset we are training is very small. \n",
        "\n",
        "<br>\n",
        "TensorFlow here by default selects the last checkpoint for translation, but if you want to check the accuracy score from a different checkpoint you can try this.\n"
      ]
    },
    {
      "metadata": {
        "id": "_xR5bAmQp1uN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "onmt-main infer --auto_config --config data.yml --features_file test.en --predictions_file output.de --checkpoint_path en_de_translation/model.ckpt-100000"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}