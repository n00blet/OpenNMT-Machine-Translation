{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTranslation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n00blet/OpenNMT-Machine-Translation/blob/master/notebook/NeuralMachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eNvH4AbDxDj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "-zqGQjlFxRwG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Neural Machine Translation using Seq2Seq Encoder-Decoder**\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Language translation has been an open problem for many years now. Even though there are many tools like Google translate, DeepL they are still evolving and far near perfect to human translation. And languages in general are vast, as in a statement can be expressed or written in more than one way. In this tutorial we will try to build a simple and basic machine translation system using [OpenNMT](http://opennmt.net/).\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "\n",
        "**Pre-Requisite **\n",
        "\n",
        "This article assumes that you have some basic knowledge in Python programming and Neural Machine Translation.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "*  Python Virtual Environment (Recommended)\n",
        "\n",
        "*  [TensorFlow](https://www.tensorflow.org/install) (GPU version)\n",
        "\n",
        "*   [CUDA](https://developer.nvidia.com/cuda-downloads) compatible Nvidia Graphics Card\n",
        "\n",
        "* [OpenNMT](https://github.com/OpenNMT/OpenNMT-tf)\n",
        "\n",
        "\n",
        "*   [Sentence Piece](https://github.com/google/sentencepiece#c-from-source)(Recommended Installation from Source)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Overview**\n",
        "\n",
        "\n",
        "*   PreProcessing the Data\n",
        "*   Training with different hyperparameters\n",
        "*   Inference or Translation\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Diving into OpenNMT**\n",
        "\n",
        "For the tasks further below, we will download one of the dataset from Machine Translation Research websites [WMT Translation Task](http://www.statmt.org/wmt16/translation-task.html)  or  [OPUS Parallel Corpus](http://opus.nlpl.eu/) .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Step 1: Preprocessing**\n",
        "\n",
        "Assuming that we have our environment ready and configured for training using GPU, the first step in this task is to preprocess the raw data.\n",
        "\n",
        "Here we build **source** and **target** word vocabularies using an Unsupervised Text Tokenizer [SentencePiece](https://github.com/google/sentencepiece). \n",
        "\n",
        "Before we go into the next task,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uu-BkvhS4NFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ontoDFPL9HdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\\\\n",
        "\n",
        "**Step 2: Training  and Evaluating  the data**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Pf4vC-BZ9LA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AfyLrQcb9Nqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 3: Traslation and Testing the results**"
      ]
    },
    {
      "metadata": {
        "id": "Ks3XEkNg9Pkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}